{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pkkj7hx_gOW"
      },
      "source": [
        "Copyright 2024 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xxj43wwPGY7_"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotnine as gg\n",
        "import requests\n",
        "\n",
        "import warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hLAHopdndC-s"
      },
      "outputs": [],
      "source": [
        "#@title Utility to load data from Cloud bucket.\n",
        "\n",
        "def import_data(path: str) -\u003e pd.DataFrame:\n",
        "  \"\"\"Load data from Cloud bucket.\"\"\"\n",
        "  base_path = 'https://storage.googleapis.com/statistical_discrimination/data'\n",
        "  full_path = f'{base_path}/{path}'\n",
        "  response = requests.get(full_path)\n",
        "  with io.BytesIO(response.content) as f:\n",
        "    return pd.read_feather(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Pp_0U7BhKEbP"
      },
      "outputs": [],
      "source": [
        "#@title Names for conditions.\n",
        "\n",
        "names = [\n",
        "    \"No memory\", \"Baseline\", \"$\\\\beta$ = 0.02\", \"$\\\\beta$ = 0.005\", \"$\\\\beta$ = 0.002\", \"$\\\\beta$ = 0.0\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfEdx0gsLa2z"
      },
      "outputs": [],
      "source": [
        "summary_df = import_data('summary_df.feather')\n",
        "analytic_df = import_data('analytic_df.feather')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96gP7OGmu7Ik"
      },
      "source": [
        "# Figure 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVIRXFOxaI4e"
      },
      "outputs": [],
      "source": [
        "summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oT6Hb5YTCeHV"
      },
      "outputs": [],
      "source": [
        "# @title Utils for plots.\n",
        "\n",
        "empirical_colors = {\n",
        "    'Random': '#333333',\n",
        "    'No memory': '#b6cbdd',\n",
        "    'Baseline': '#497dab',\n",
        "    '$\\\\beta$ = 0.02': 'tab:brown',\n",
        "    '$\\\\beta$ = 0.005': 'tab:orange',\n",
        "    '$\\\\beta$ = 0.002': 'tab:red',\n",
        "    '$\\\\beta$ = 0.0': 'tab:pink',\n",
        "}\n",
        "race_colors = {'2': '#49ab7d', '8': '#ab497d'}\n",
        "# The palette to show different conditions as different colors.\n",
        "palette = {\n",
        "    'Condition': empirical_colors,\n",
        "    '# Races': race_colors,\n",
        "}\n",
        "\n",
        "\n",
        "def make_discrimination_plot(display_name: str, df: pd.DataFrame):\n",
        "  \"\"\"Make a discrimination vs community bias plot.\n",
        "\n",
        "  Args:\n",
        "    display_name: The name of the plot, this must be one of the values in\n",
        "      `names` which also correspond to the `arch` column in `df`.\n",
        "    df: The dataframe containing data to plot.\n",
        "  \"\"\"\n",
        "  # Branch copy of summary dataframe.\n",
        "  plot_df = df.loc[df.arch == display_name].copy()\n",
        "  # Normalize participation for use with alpha aesthetic.\n",
        "  plot_df['norm_part'] = plot_df['part'] / plot_df['part'].max()\n",
        "  # Format labels for `race` and `k` variables\n",
        "  plot_df['race_label'] = plot_df['race'].map('{} race'.format).str.capitalize()\n",
        "  plot_df['k_label'] = plot_df['k'].map('{} races'.format).str.capitalize()\n",
        "  # Visualize.\n",
        "  plot = (\n",
        "      gg.ggplot(data = plot_df)\n",
        "      + gg.geom_abline(intercept=0, slope=0, colour='#555555', size=1)\n",
        "      + gg.aes(x = 'corr',\n",
        "               y = 'discr')\n",
        "      + gg.geom_jitter(colour=empirical_colors[display_name],\n",
        "                       width = 0.05,\n",
        "                       height = 0.05,\n",
        "                       show_legend = False)\n",
        "      + gg.geom_smooth(method = 'lm',\n",
        "                       color = 'red')\n",
        "      + gg.facet_grid('k_label ~ race_label')\n",
        "      + gg.labs(x = f'Color-strategy correlation',\n",
        "                y = 'Discrimination index')\n",
        "      + gg.ylim(-0.6, 0.6)\n",
        "      + gg.theme_bw()\n",
        "      + gg.theme(figure_size = [3, 1.5 * len(df.k.unique())],\n",
        "                axis_title = gg.element_text(size = 10),\n",
        "                axis_text = gg.element_text(size = 8),\n",
        "                strip_background = gg.element_rect(fill = 'white'),\n",
        "                strip_text = gg.element_text(size = 8)))\n",
        "  print(plot)\n",
        "  # Output svg file.\n",
        "  dataname = f'{display_name.replace(\" \", \"_\").lower()}'\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore', category = UserWarning)\n",
        "    plot_name = f'{dataname}.svg'\n",
        "    plot.save(plot_name, format = 'svg')\n",
        "\n",
        "\n",
        "def get_discrimination_index_for_run(array: np.ndarray) -\u003e np.ndarray:\n",
        "  \"\"\"Get the discrimination index for a single run (set of agents).\n",
        "\n",
        "  Args:\n",
        "    array: The array to get the discrimination index from. The array must be\n",
        "      indexed by the 4 possible types of partners: purple cooperator, purple\n",
        "      defector, teal cooperator, and teal defector. A second, optional, index\n",
        "      can index over agents in the run.\n",
        "\n",
        "  Returns:\n",
        "    The discrimination index for the run. If multiple agents were given, the\n",
        "    returned value is indexed by agent, otherwise is a scalar.\n",
        "  \"\"\"\n",
        "  v_discr = np.abs(array[0] - array[1]) + np.abs(array[2] - array[3])\n",
        "  b_discr = np.abs(array[0] - array[2]) + np.abs(array[1] - array[3])\n",
        "  denominator = np.where(array.sum() == 0, 1, array.sum())\n",
        "  return (b_discr - v_discr) / denominator\n",
        "\n",
        "\n",
        "def bootstrap_95(series: pd.Series,\n",
        "                 bootstrap_count: int = 5000,\n",
        "                 verbose: bool = False) -\u003e tuple[float, float]:\n",
        "  \"\"\"Bootstraps the discrimination index for a series.\n",
        "\n",
        "  This function is used to calculate 95% confidence intervals via bootstrap on\n",
        "  the participation counts in a run. The provided series must have the\n",
        "  participation counts (i.e. containing number of times paired with purple\n",
        "  cooperator, purple defector, teal cooperator, and teal defector). Bootstraping\n",
        "  is computed by sampling from the participation counts using a multinomial\n",
        "  distribution.\n",
        "\n",
        "  Args:\n",
        "    series: The series to bootstrap. Must contain participation counts.\n",
        "    bootstrap_count: The number of bootstrap iterations to run.\n",
        "    verbose: Whether to print the bootstrap confidence intervals and other\n",
        "      useful information.\n",
        "\n",
        "  Returns:\n",
        "    The bootstrapped discrimination index for the series as a pair of values\n",
        "    corresponding to the 2.5% and 97.5% percentiles of the discrimination index.\n",
        "  \"\"\"\n",
        "  bootstrapped = []\n",
        "  for _ in range(bootstrap_count):\n",
        "    sampled = series.apply(\n",
        "        lambda x: get_discrimination_index_for_run(\n",
        "            np.random.multinomial(x.sum(), x/x.sum())))\n",
        "    bootstrapped.append(np.mean(sampled))\n",
        "  srt = sorted(bootstrapped)\n",
        "  srt_btstrp_2_5 = srt[int(0.025*len(bootstrapped))]\n",
        "  srt_btstrp_97_5 = srt[int(0.975*len(bootstrapped))]\n",
        "  if verbose:\n",
        "    print('mean', np.mean(series.values), 'std.dev', np.std(series.values),\n",
        "          'std.err', np.std(series.values) / np.sqrt(len(series.values)),\n",
        "          'mean + std.err', np.mean(series.values) + np.std(series.values) / np.sqrt(len(series.values)),\n",
        "          'bootstrapped 2.5%-ile', srt_btstrp_2_5,\n",
        "          'bootstrapped 97.5%-ile', srt_btstrp_97_5, \"series\", list(series.values))\n",
        "  return srt_btstrp_2_5, srt_btstrp_97_5\n",
        "\n",
        "\n",
        "def average_discrimination_over_bias(df: pd.DataFrame,\n",
        "                                     fill: str = 'Condition',\n",
        "                                     bootstrap: bool = True):\n",
        "  \"\"\"Makes a plot for the average discrimination across community bias.\n",
        "\n",
        "  Args:\n",
        "    df: The dataframe with summary data.\n",
        "    fill: The fill variable to use for filling the bars of the plot.\n",
        "    bootstrap: Whether to bootstrap the discrimination index.\n",
        "  \"\"\"\n",
        "  grouped = df.groupby(['arch', 'k', 'race'])\n",
        "  plot_df = grouped.mean()\n",
        "  if bootstrap:\n",
        "    tmp = grouped['raw_discr_counts'].aggregate(bootstrap_95)\n",
        "    plot_df['discr_2_5ci'] = tmp.apply(lambda x: x[0])\n",
        "    plot_df['discr_97_5ci'] = tmp.apply(lambda x: x[1])\n",
        "  else:\n",
        "    tmp = grouped['discr'].aggregate(\n",
        "        lambda x: np.std(x.values) / np.sqrt(len(x.values))\n",
        "    )\n",
        "    plot_df['discr_2_5ci'] = plot_df['discr'] - tmp\n",
        "    plot_df['discr_97_5ci'] = plot_df['discr'] + tmp\n",
        "  plot_df = plot_df.reset_index()\n",
        "  # Format labels for `race` and `k` variables\n",
        "  plot_df['race_label'] = plot_df['race'].map('{} race'.format).str.capitalize()\n",
        "  plot_df['k_label'] = plot_df['k'].map('{} races'.format)\n",
        "  plot_df['Condition'] = plot_df['arch']\n",
        "  plot_df['Condition'] = pd.Categorical(\n",
        "      plot_df['Condition'],\n",
        "      categories=names,\n",
        "      ordered=True)\n",
        "  # Visualize.\n",
        "  plot = (\n",
        "      gg.ggplot(data = plot_df)\n",
        "      + gg.aes(x = 'Condition',\n",
        "               y = 'discr', fill=fill)\n",
        "      + gg.geom_abline(intercept=0, slope=0, colour='#555555', size=1)\n",
        "      + gg.geom_bar(stat='identity')\n",
        "      + gg.geom_errorbar(gg.aes(ymin='discr_2_5ci',\n",
        "                                ymax='discr_97_5ci'),\n",
        "      )\n",
        "      + gg.facet_grid('k_label ~ race_label')\n",
        "      + gg.labs(x = 'Condition',\n",
        "                y = 'Avg. discr. index')\n",
        "      + gg.theme_bw()\n",
        "      + gg.theme(\n",
        "          figure_size = [2.2 * len(plot_df.race.unique()),\n",
        "                         2.5 * len(df.k.unique())],\n",
        "          axis_title = gg.element_text(size=10),\n",
        "          axis_text = gg.element_text(size=8),\n",
        "          strip_background = gg.element_rect(fill = 'white'),\n",
        "          strip_text = gg.element_text(size=9),\n",
        "          axis_text_x = gg.element_text(angle=45, vjust=1, hjust=1),\n",
        "      )\n",
        "      + gg.scale_fill_manual(palette[fill])\n",
        "  )\n",
        "\n",
        "  print(plot)\n",
        "  # Output SVG file.\n",
        "  dataname = 'avg_discr_vs_percept'\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore', category = UserWarning)\n",
        "    plot_name = f'{dataname}.svg'\n",
        "    plot.save(plot_name, format = 'svg')\n",
        "\n",
        "\n",
        "def make_reward_plot(df: pd.DataFrame, fill: str = 'Condition'):\n",
        "  \"\"\"Makes a plot showing the rewards for all conditions.\n",
        "\n",
        "  Args:\n",
        "    df: The dataframe with summary data.\n",
        "    fill: The fill variable to use for filling the bars of the plot.\n",
        "  \"\"\"\n",
        "  grouped = df.loc[df['race'] == 'last'].groupby(\n",
        "      ['arch', 'k', 'race'])\n",
        "  plot_df = grouped.mean()\n",
        "  plot_df['rwd_std'] = grouped['rwd'].std() / np.sqrt(grouped['rwd'].count())\n",
        "  plot_df = plot_df.reset_index()\n",
        "  # Format labels for `race` and `k` variables\n",
        "  plot_df['k_label'] = plot_df['k'].map('{} races'.format)\n",
        "  plot_df['# Races'] = plot_df['k'].map('{}'.format)\n",
        "  plot_df['Condition'] = plot_df['arch']\n",
        "  plot_df['Condition'] = pd.Categorical(\n",
        "      plot_df['Condition'],\n",
        "      categories=names,\n",
        "      ordered=True)\n",
        "  # Visualize.\n",
        "  plot = (\n",
        "      gg.ggplot(data = plot_df)\n",
        "      + gg.aes(x = 'Condition',\n",
        "               y = 'rwd', fill=fill)\n",
        "      + gg.geom_col(stat='identity', position='dodge',\n",
        "                    )\n",
        "      + gg.geom_errorbar(gg.aes(ymin='rwd - rwd_std',\n",
        "                                ymax='rwd + rwd_std'),\n",
        "                         )\n",
        "      + gg.labs(x = f'Condition',\n",
        "                y = 'Episode reward')\n",
        "      + gg.theme_bw()\n",
        "      + gg.theme(\n",
        "          figure_size = [3.8, 3],\n",
        "          axis_title = gg.element_text(size=10),\n",
        "          axis_text = gg.element_text(size=8),\n",
        "          strip_background = gg.element_rect(fill = 'white'),\n",
        "          strip_text = gg.element_text(size=9),\n",
        "          axis_text_x = gg.element_text(angle=45, vjust=1, hjust=1),\n",
        "      )\n",
        "      + gg.scale_fill_manual(palette[fill])\n",
        "  )\n",
        "\n",
        "  print(plot)\n",
        "  # Output SVG file.\n",
        "  dataname = 'avg_rwd_vs_percept'\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore', category = UserWarning)\n",
        "    plot_name = f'{dataname}.svg'\n",
        "    plot.save(plot_name, format = 'svg')\n",
        "\n",
        "\n",
        "def make_attribute_plot_all(attribute: str, df: pd.DataFrame, fill='Condition'):\n",
        "  \"\"\"Makes a plot of awareness or stickiness for all conditions.\"\"\"\n",
        "  label = {\n",
        "      'stickiness_prob': 'Stickiness',\n",
        "      'awareness_prob': 'Awareness',\n",
        "  }\n",
        "  random = {\n",
        "      'stickiness_prob': 0.2,\n",
        "      'awareness_prob': 0.5,\n",
        "  }\n",
        "  # Branch copy of summary dataframe.\n",
        "  plot_df = df.copy()\n",
        "  del plot_df['raw_discr_counts']\n",
        "  # Stickiness \u0026 Awareness only make sense for computation at the last race.\n",
        "  plot_df = plot_df[plot_df['race']=='last']\n",
        "  del plot_df['race']\n",
        "  grouped = plot_df.groupby(['arch', 'k'])\n",
        "  plot_df = grouped.mean()\n",
        "  plot_df[f'{attribute}_std'] = grouped[attribute].std() / np.sqrt(grouped[attribute].count())\n",
        "  plot_df = plot_df.reset_index()\n",
        "  # Normalize participation for use with alpha aesthetic.\n",
        "  plot_df['norm_part'] = plot_df['part'] / plot_df['part'].max()\n",
        "  # Format labels for `k` variable\n",
        "  plot_df['# Races'] = plot_df['k'].map('{}'.format)\n",
        "  plot_df['Condition'] = plot_df['arch']\n",
        "  plot_df['Condition'] = pd.Categorical(\n",
        "      plot_df['Condition'],\n",
        "      categories=names,\n",
        "      ordered=True)\n",
        "  # Visualize.\n",
        "  plot = (\n",
        "      gg.ggplot(data = plot_df)\n",
        "      + gg.aes(x='Condition', y=attribute, fill=fill)\n",
        "      + gg.geom_col(stat='identity', position='dodge')\n",
        "      + gg.geom_errorbar(gg.aes(ymin=f'{attribute}-{attribute}_std',\n",
        "                                ymax=f'{attribute}+{attribute}_std'),\n",
        "                         position=gg.position_dodge(0.9))\n",
        "      + gg.labs(x = 'Condition',\n",
        "                y = label[attribute])\n",
        "      + gg.theme_bw()\n",
        "      + gg.theme(figure_size = [3.8, 3],\n",
        "                axis_title = gg.element_text(size = 10),\n",
        "                axis_text = gg.element_text(size = 8),\n",
        "                axis_text_x = gg.element_text(angle=45, vjust=1, hjust=1),\n",
        "                strip_background = gg.element_rect(fill='white'),\n",
        "                strip_text = gg.element_text(size = 10))\n",
        "      + gg.geom_abline(intercept=random[attribute], slope=0, size=1, colour=empirical_colors['Random'], show_legend=True)\n",
        "      + gg.labs(colour=\"\")\n",
        "      + gg.scale_fill_manual(palette[fill])\n",
        "  )\n",
        "\n",
        "  print(plot)\n",
        "\n",
        "  dataname = f'{attribute}'\n",
        "  plot_name = f'{dataname}.svg'\n",
        "  plot.save(plot_name, format = 'svg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnBCj2j4qXVf"
      },
      "outputs": [],
      "source": [
        "make_discrimination_plot('Baseline', summary_df.loc[summary_df.k == 8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzI_gtVRqpUX"
      },
      "outputs": [],
      "source": [
        "make_discrimination_plot('$\\\\beta$ = 0.0', summary_df.loc[summary_df.k == 8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIpVsgl7YVb2"
      },
      "outputs": [],
      "source": [
        "average_discrimination_over_bias(summary_df.loc[summary_df.k == 8], bootstrap=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAmfjWEZNDfx"
      },
      "outputs": [],
      "source": [
        "average_discrimination_over_bias(summary_df.loc[summary_df.k == 8], bootstrap=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQK2XzRA-wV7"
      },
      "source": [
        "# Analytical model \u0026 Fig 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbfW7SOGsBTH"
      },
      "outputs": [],
      "source": [
        "make_attribute_plot_all('awareness_prob', summary_df.loc[summary_df.k == 8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6sb9e-AsGnX"
      },
      "outputs": [],
      "source": [
        "make_attribute_plot_all('stickiness_prob', summary_df.loc[summary_df.k == 8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikr7JmEkiw_x"
      },
      "outputs": [],
      "source": [
        "make_reward_plot(summary_df.loc[summary_df.k == 8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FGMX6KVJtEU3"
      },
      "outputs": [],
      "source": [
        "#@title Create empirical CSV to overlay\n",
        "\n",
        "plot_df = summary_df.copy()\n",
        "# Stickiness \u0026 Awareness only make sense for computation at the last race.\n",
        "plot_df = plot_df[plot_df['race']=='last']\n",
        "del plot_df['race']\n",
        "grouped = plot_df.groupby(['arch', 'k'])\n",
        "plot_df = grouped.mean()\n",
        "plot_df = plot_df.reset_index()\n",
        "plot_df['arch_label'] = plot_df['arch']  # .map(lambda x: x.split(' ')[-1] if x != 'A3C LargeNet' else 'Baseline')\n",
        "plot_df['arch_label'] = pd.Series(plot_df['arch_label'], dtype=\"category\")\n",
        "empirical_csv = plot_df[['arch_label', 'awareness_prob', 'stickiness_prob', 'k']].to_csv()\n",
        "empirical_csv += '12,Random,0.5,0.2,2\\n13,Random,0.5,0.2,8\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lxAikx89tH8F"
      },
      "outputs": [],
      "source": [
        "#@title Plot Analytical figure\n",
        "\n",
        "plot_df = analytic_df.copy()\n",
        "del plot_df['race_counts']  # Not needed, prevents aggregation\n",
        "grouped_df = plot_df.groupby(['k', 'awareness', 'stickiness'])\n",
        "grouped_df = grouped_df.mean()\n",
        "grouped_df = grouped_df.reset_index()\n",
        "grouped_df = grouped_df.loc[(grouped_df.k == 2) | (grouped_df.k == 8)]\n",
        "ks = sorted(set(grouped_df.k))\n",
        "\n",
        "Xs = sorted(set(grouped_df.stickiness))[1:5]\n",
        "Ys = sorted(set(grouped_df.awareness))[4:8]\n",
        "Xv, Yv = np.meshgrid(Xs, Ys)\n",
        "\n",
        "sticks = Xs\n",
        "\n",
        "fig, axes = plt.subplots(1, len(ks), tight_layout=True)\n",
        "fig.set_size_inches(3*len(ks) + 1.5, 3)\n",
        "\n",
        "axes[0].set_ylabel('Awareness $\\omega$')\n",
        "\n",
        "csv_io = io.StringIO(empirical_csv)\n",
        "empirical_df = pd.read_csv(csv_io).reset_index(drop=True)\n",
        "\n",
        "for j, k in enumerate(ks):\n",
        "  last_fn = np.vectorize(\n",
        "      lambda x, y: grouped_df[(grouped_df.awareness == y) \u0026 (grouped_df.stickiness == x) \u0026\n",
        "                        (grouped_df.k == k)].last_race_discr.mean())\n",
        "\n",
        "  empirical_subset = empirical_df.loc[empirical_df.k == k]\n",
        "  last_v = -last_fn(Xv, Yv)\n",
        "\n",
        "  axes[j].set_title(f'${k}$ races')\n",
        "\n",
        "  axes[j].set_xlabel('Stickiness $s$')\n",
        "\n",
        "  cs = axes[j].contourf(Xv, Yv, last_v,\n",
        "                        extend='both')\n",
        "  cbar = fig.colorbar(cs, ax=axes[j], format='%.1f')\n",
        "\n",
        "  axes[j].scatter(empirical_subset.stickiness_prob,\n",
        "                  empirical_subset.awareness_prob,\n",
        "                  c=empirical_subset.arch_label.map(empirical_colors),\n",
        "                  edgecolors='#555555')\n",
        "  handles = [\n",
        "      plt.Line2D([0], [0], marker='o', color='w', markeredgecolor='#555555',\n",
        "                 markerfacecolor=v, label=k, markersize=8)\n",
        "      for k, v in empirical_colors.items()\n",
        "  ]\n",
        "\n",
        "axes[-1].legend(title='Condition', handles=handles,\n",
        "                bbox_to_anchor=(1.4, 1),\n",
        "                loc='upper left')\n",
        "\n",
        "dataname = 'analytic_w_overlay'\n",
        "plot_name = f'{dataname}.svg'\n",
        "fig.savefig(plot_name, format = 'svg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mX_4zIEq9U4"
      },
      "source": [
        "# Supplementary Material Figures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3gcaV-PXnIW"
      },
      "outputs": [],
      "source": [
        "make_reward_plot(summary_df, fill='# Races')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9wPCbwg5pGR"
      },
      "outputs": [],
      "source": [
        "average_discrimination_over_bias(summary_df, bootstrap=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC-25QCn74ui"
      },
      "outputs": [],
      "source": [
        "make_discrimination_plot('Baseline', summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vYF79NRvhBv"
      },
      "outputs": [],
      "source": [
        "make_discrimination_plot('No memory', summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9Yk58qrbo5-"
      },
      "outputs": [],
      "source": [
        "make_discrimination_plot('$\\\\beta$ = 0.0', summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U73mwey4K2gQ"
      },
      "outputs": [],
      "source": [
        "make_discrimination_plot('$\\\\beta$ = 0.002', summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY_Q_VVQTuWZ"
      },
      "outputs": [],
      "source": [
        "make_discrimination_plot('$\\\\beta$ = 0.005', summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ0_1kannNe1"
      },
      "outputs": [],
      "source": [
        "make_discrimination_plot('$\\\\beta$ = 0.02', summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AyXm35qoCZq5"
      },
      "outputs": [],
      "source": [
        "# @title Run regressions\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "# Initialize consistuent lists for dataframe.\n",
        "agent_architectures = []\n",
        "overall_num_races = []\n",
        "race_num = []\n",
        "inters = []\n",
        "archs = []\n",
        "all_coeffs = []\n",
        "all_ci_95_lower = []\n",
        "all_ci_95_upper = []\n",
        "# Group dataframe by number of races and current race number.\n",
        "summary_df_groups = summary_df.groupby(['k', 'race', 'arch'])\n",
        "# Iterate through groups.\n",
        "for i, ((k, race, arch), grouped_df) in enumerate(summary_df_groups):\n",
        "  # Set up predictors and outcome variable.\n",
        "  X = grouped_df['corr']\n",
        "  X = sm.add_constant(X)\n",
        "  Y = grouped_df['discr']\n",
        "  # Fit model.\n",
        "  model = sm.OLS(Y, X)\n",
        "  res = model.fit()\n",
        "  # Extract coefficient and confidence intervals.\n",
        "  coeff = res.params['corr']\n",
        "  inter = res.params['const'] if 'const' in res.params else 0.0\n",
        "  ci_95_lower, ci_95_upper = res.conf_int().loc['corr']\n",
        "  # Record this group's params and results.\n",
        "  overall_num_races.append(k)\n",
        "  race_num.append(race)\n",
        "  archs.append(arch)\n",
        "  all_coeffs.append(coeff)\n",
        "  all_ci_95_lower.append(ci_95_lower)\n",
        "  all_ci_95_upper.append(ci_95_upper)\n",
        "  inters.append(inter)\n",
        "discr_results_df = pd.DataFrame({\n",
        "    'Description': archs,\n",
        "    'Race_Num': race_num,\n",
        "    'Total_Num_Races': overall_num_races,\n",
        "    'Coeff': all_coeffs,\n",
        "    'CI_95%_low': all_ci_95_lower,\n",
        "    'CI_95%_upp': all_ci_95_upper,\n",
        "    'Intercept': inters,\n",
        "}).sort_values(['Description', 'Total_Num_Races']).reset_index(drop = True)\n",
        "discr_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iV669sCMW9ZC"
      },
      "outputs": [],
      "source": [
        "#@title Crown decay plot {run: 'auto'}\n",
        "\n",
        "# Good values are:\n",
        "# * 0.0 (no decay)\n",
        "# * 0.002 (mid association decay)\n",
        "# * 0.005 (mid reward decay)\n",
        "# * 0.02 (early reward decay)\n",
        "\n",
        "alpha = 0.2\n",
        "min_thresh = 0.3\n",
        "max_thresh = 0.6\n",
        "\n",
        "cooldown = 2\n",
        "race_duration = 225\n",
        "partner_duration = 75\n",
        "duration = race_duration + partner_duration\n",
        "\n",
        "beta = 0.002 #@param {type:\"slider\", min:0, max:0.1, step:0.001}\n",
        "\n",
        "xs = list(range(duration))\n",
        "rowing = ([1] + [0]*cooldown)*13\n",
        "rowing += [0] * (duration - len(rowing))\n",
        "ys = [0]\n",
        "for r, x in zip(rowing, xs):\n",
        "  y = ys[-1]\n",
        "  if r:\n",
        "    y = alpha * r + (1-alpha) * y\n",
        "  y *= 1 - beta\n",
        "  ys.append(y)\n",
        "\n",
        "ys = ys[1:]\n",
        "\n",
        "plt.plot(xs, ys)\n",
        "plt.plot(xs, [min_thresh]*duration)\n",
        "plt.plot(xs, [max_thresh]*duration);\n",
        "plt.plot(xs, [0]*race_duration + [1]*partner_duration);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZszBK-T0iLK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Data analysis - PNAS.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
